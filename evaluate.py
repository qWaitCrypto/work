import json
import asyncio
import aiohttp
import os
from bert_score import score

# ç³»ç»Ÿæç¤ºè¯
system_prompt = """You are an AI assistant trained to provide precise, concise, and well-formatted answers based on the type of question asked. Follow these rules:

1. **For factual, mathematical, and logical reasoning questions**:
   - Provide only the final answer without explanations.
   - Example:
     User: "What is 2 + 2?"
     AI: "4"

2. **For programming and code generation tasks**:
   - Provide a complete and correct code snippet.
   - Do not include explanations, just the code.
   - Example:
     User: "Write a Python function to reverse a list."
     AI:
     ```
     def reverse_list(lst):
         return lst[::-1]
     ```

3. **For multi-step logical reasoning problems**:
   - Keep the reasoning minimal but clear.
   - Summarize the thought process in 1-2 sentences before giving the answer.
   - Example:
     User: "If A > B and B > C, what is the relation between A and C?"
     AI: "Since A is greater than B, and B is greater than C, it follows that A > C."

4. **For reading comprehension and open-ended questions**:
   - Answer in a structured but concise manner.
   - Keep responses within 2-3 sentences unless explicitly requested for more details.

5. **General Rules**:
   - If the answer is unknown, reply: "I don't know."
   - Always use clear and formatted output.
   - If a question expects a list, format the answer as a list.
"""

# API é…ç½®
API_URL = "http://122.191.109.151:1112/v1/chat/completions"
API_KEY = "sk-f267b40f68fe47fbba06d9534b988214"

# è¯»å–æ•°æ®é›†
def load_dataset(path, limit=None):
    """åŠ è½½æ•°æ®é›†å¹¶å¯é€‰æ‹©é™åˆ¶æ ·æœ¬æ•°é‡"""
    with open(path, "r", encoding="utf-8") as f:
        dataset = json.load(f)
    
    if limit and limit > 0:
        dataset = dataset[:limit]
    
    return dataset

# æå–æœŸæœ›ç­”æ¡ˆ
def extract_expected_answers(expected):
    """æå–æœŸæœ›ç­”æ¡ˆï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
    if isinstance(expected, dict) and "text" in expected:
        return expected["text"]
    elif isinstance(expected, list):
        return expected
    return [str(expected)]

# è®¡ç®—BERTScore
def compute_bertscore(pred, expected):
    """è®¡ç®—BERTScoreè¯­ä¹‰åŒ¹é…åˆ†æ•°"""
    try:
        # ç¡®ä¿è¾“å…¥æ˜¯åˆ—è¡¨æ ¼å¼
        if not isinstance(pred, list):
            pred = [pred]
        if not isinstance(expected, list):
            expected = [expected]
        
        # è®¡ç®—BERTScore
        P, R, F1 = score(pred, expected, model_type="bert-base-uncased", verbose=False)
        
        # è¿”å›F1åˆ†æ•°ï¼ˆé€šå¸¸è¢«è§†ä¸ºBERTScoreçš„ä¸»è¦æŒ‡æ ‡ï¼‰
        return F1.mean().item()
    except Exception as e:
        print(f"âŒ BERTScoreè®¡ç®—å¤±è´¥: {e}")
        return 0

# è¯„ä¼°æ¨¡å‹è¾“å‡º
def evaluate_answer(pred, expected):
    """ä½¿ç”¨BERTScoreè¯„ä¼°æ¨¡å‹è¾“å‡ºä¸æœŸæœ›ç­”æ¡ˆçš„åŒ¹é…åº¦"""
    possible_answers = extract_expected_answers(expected)
    pred_clean = pred.strip()
    
    # è®¡ç®—ä¸æ‰€æœ‰å¯èƒ½ç­”æ¡ˆçš„BERTScoreï¼Œå–æœ€é«˜å€¼
    bert_scores = [compute_bertscore(pred_clean, ans) for ans in possible_answers]
    final_score = max(bert_scores) if bert_scores else 0
    
    return round(final_score, 4)

# å¼‚æ­¥è¯·æ±‚å‡½æ•°
async def query_model(session, question):
    """å‘APIå‘é€è¯·æ±‚è·å–æ¨¡å‹å›ç­”"""
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}"
    }
    payload = {
        "model": "deepseek-chat",
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": question}
        ],
        "max_tokens": 256
    }
    
    try:
        async with session.post(API_URL, json=payload, headers=headers) as response:
            if response.status == 200:
                result = await response.json()
                return result.get("choices", [{}])[0].get("message", {}).get("content", "").strip()
            else:
                error_text = await response.text()
                print(f"âŒ APIè¯·æ±‚å¤±è´¥ (çŠ¶æ€ç : {response.status}): {error_text}")
                return f"Error: APIè¯·æ±‚å¤±è´¥ (çŠ¶æ€ç : {response.status})"
    except Exception as e:
        print(f"âŒ APIè¯·æ±‚å¼‚å¸¸: {e}")
        return f"Error: {str(e)}"

# è¿è¡Œè¯„ä¼°
async def run_evaluation(dataset_path, limit=None, output_prefix="evaluation"):
    """è¿è¡Œè¯„ä¼°æµç¨‹"""
    print(f"ğŸ“Š å¼€å§‹è¯„ä¼°æµç¨‹...")
    
    # åŠ è½½æ•°æ®é›†
    dataset = load_dataset(dataset_path, limit)
    print(f"âœ… å·²åŠ è½½æ•°æ®é›†ï¼Œå…±{len(dataset)}ä¸ªæ ·æœ¬")
    
    results = []
    total_score = 0
    
    # åˆ›å»ºHTTPä¼šè¯
    async with aiohttp.ClientSession() as session:
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = []
        for sample in dataset:
            task = asyncio.create_task(query_model(session, sample["question"]))
            tasks.append((sample, task))
        
        # å¤„ç†ç»“æœ
        for i, (sample, task) in enumerate(tasks):
            try:
                print(f"â³ å¤„ç†æ ·æœ¬ {i+1}/{len(tasks)}...")
                response = await task
                
                # è¯„ä¼°å›ç­”
                score = evaluate_answer(response, sample["expected_answer"])
                total_score += score
                
                # è®°å½•ç»“æœ
                results.append({
                    "id": sample.get("id", i+1),
                    "question": sample["question"],
                    "expected_answer": sample["expected_answer"],
                    "model_response": response,
                    "bert_score": score
                })
                
                print(f"âœ… æ ·æœ¬ {i+1} è¯„åˆ†: {score:.4f}")
            except Exception as e:
                print(f"âŒ å¤„ç†æ ·æœ¬ {i+1} æ—¶å‡ºé”™: {e}")
    
    # è®¡ç®—å¹³å‡åˆ†æ•°
    avg_score = total_score / len(results) if results else 0
    
    # åˆ›å»ºæ€»ç»“æŠ¥å‘Š
    summary = {
        "total_samples": len(results),
        "average_bert_score": round(avg_score, 4),
        "evaluation_time": "å®Œæˆ"
    }
    
    # ä¿å­˜ç»“æœ
    results_file = f"{output_prefix}_results.json"
    summary_file = f"{output_prefix}_summary.json"
    
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=4, ensure_ascii=False)
    
    with open(summary_file, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=4, ensure_ascii=False)
    
    print(f"âœ… è¯„ä¼°å®Œæˆï¼")
    print(f"ğŸ“Š å¹³å‡BERTScore: {avg_score:.4f}")
    print(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {results_file}")
    print(f"ğŸ“„ è¯„ä¼°æ‘˜è¦å·²ä¿å­˜è‡³: {summary_file}")

# ä¸»å‡½æ•°
if __name__ == "__main__":
    # é…ç½®å‚æ•°
    dataset_path = "./evaluation_dataset.json"
    sample_limit = 100  # è®¾ç½®ä¸ºNoneå¯è¯„ä¼°æ•´ä¸ªæ•°æ®é›†
    
    # è¿è¡Œè¯„ä¼°
    asyncio.run(run_evaluation(dataset_path, sample_limit))

